# Server Configuration
PORT=3000
HOST=localhost

# API Keys
OPENAI_API_KEY=sk-your-openai-key-here
OPENAI_BASE_URL=https://api.openai.com/v1  # Optional: custom OpenAI endpoint
ANTHROPIC_API_KEY=sk-ant-your-anthropic-key-here

# Ollama (local LLM server)
OLLAMA_BASE_URL=http://localhost:11434/v1
OLLAMA_MODEL=llama2  # or llama3, mistral, etc.
OLLAMA_API_KEY=not-needed  # Optional: usually not required

# LM Studio (local LLM server)
LMSTUDIO_BASE_URL=http://localhost:1234/v1
LMSTUDIO_MODEL=local-model  # Set to your loaded model
LMSTUDIO_API_KEY=not-needed  # Optional: usually not required

# Generic OpenAI-compatible provider
OPENAI_COMPATIBLE_BASE_URL=https://your-provider.com/v1
OPENAI_COMPATIBLE_MODEL=your-model-name
OPENAI_COMPATIBLE_API_KEY=your-api-key  # Optional: if required by provider

# Authentication (Optional - set AUTH_ENABLED=true to enable)
AUTH_ENABLED=false
AUTH_REQUIRE_ORG=false
# API_KEYS format: key:organizationId:userId (comma-separated)
API_KEYS=test-key-123:org-abc:user-1,prod-key-456:org-xyz:user-2

# CORS Configuration
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:5173

# Git Storage
CONVERSATIONS_DIR=../conversations

# Streaming Configuration
STREAMING_TIMEOUT=300
MAX_CONCURRENT_STREAMS=10

# Git Optimization
GIT_CACHE_SIZE=1000
GIT_SHALLOW_CLONE=true

# Data Retention (days, 0 = infinite)
RETENTION_DAYS=0

# Logging
LOG_LEVEL=info
